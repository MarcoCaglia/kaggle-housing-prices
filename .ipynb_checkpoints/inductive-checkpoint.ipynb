{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: House Prices: Advanced Regression Techniques\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n",
    "## Part 1: Machine Learning\n",
    "### Outline:\n",
    "1. Preparation\n",
    "2. Testing and Selecting Base Models\n",
    "3. Finetuning of the Best Model\n",
    "4. Comparison with simple DLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, Normalizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from kaggle_scorer import rmsle, rmsle_validation\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: Import processed dataset\n",
    "path = 'train_processed.csv'\n",
    "df = pd.read_csv(path, index_col='Id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: Split the dataset in train and test data\n",
    "X = np.array(df.drop('SalePrice_log',axis=1))\n",
    "y = np.array(df.loc[:,'SalePrice_log'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.80,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation: Prepare Pipelines\n",
    "poly_fit = PolynomialFeatures(degree=2)\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "norm = Normalizer()\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "rand_for = RandomForestRegressor()\n",
    "ada_boost = AdaBoostRegressor()\n",
    "grad_boost = GradientBoostingRegressor()\n",
    "\n",
    "lin_pipe = Pipeline([('OneHotEncoder',encoder),\n",
    "                     ('Normalizer',norm),\n",
    "                     ('StandardScaler',scaler),\n",
    "                     ('LinearRegression',lin_reg)])\n",
    "\n",
    "rand_for_pipe = Pipeline([('OneHotEncoder',encoder),\n",
    "                          ('Normalizer',norm),\n",
    "                          ('StandardScaler',scaler),\n",
    "                          ('RandomForest',rand_for)])\n",
    "\n",
    "ada_boost_pipe = Pipeline([('OneHotEncoder',encoder),\n",
    "                           ('Normalizer',norm),\n",
    "                           ('StandardScaler',scaler),\n",
    "                           ('AdaBoost',ada_boost)])\n",
    "\n",
    "grad_boost_pipe = Pipeline([('OneHotEncoder',encoder),\n",
    "                            ('Normalizer',norm),\n",
    "                            ('StandardScaler',scaler),\n",
    "                            ('GradientBoosting',grad_boost)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Selecting Base Models: Defining test function\n",
    "def test_model(pipeline,X_train=X_train,X_test=X_test,y_train=y_train,y_test=y_test):\n",
    "    \n",
    "    start_time = dt.datetime.now()\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    end_time = dt.datetime.now()\n",
    "    fit_time = dt.timedelta.total_seconds(end_time-start_time)\n",
    "    \n",
    "    yhat_train = pipeline.predict(X_train)\n",
    "    train_score = rmsle(y_train,yhat_train)\n",
    "    \n",
    "    yhat = pipeline.predict(X_test)\n",
    "    test_score = rmsle(y_test,yhat)\n",
    "    \n",
    "    return np.array((fit_time,train_score,test_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Selecting Base Models: Testing\n",
    "pipe_list = [lin_pipe,rand_for_pipe,ada_boost_pipe,grad_boost_pipe]\n",
    "results = np.empty((4,3))\n",
    "\n",
    "test_map = map(test_model,pipe_list)\n",
    "i = 0\n",
    "for result in test_map:\n",
    "    results[i]=result\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and Selecting Base Models: Visualize Test Results\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "names = ['Linear','RFR','ABR','GBR']\n",
    "titles = ['Training Time','Training Accuracy','Test Accuracy']\n",
    "y_titles = ['Seconds','RMSLE','RMSLE']\n",
    "plot_info = zip(titles,y_titles)\n",
    "\n",
    "for i,titles in enumerate(plot_info):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.bar(x=names,height=results[:,i])\n",
    "    plt.title(titles[0])\n",
    "    plt.ylabel(titles[1])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent, that the linear model tales the least training time, thanks to its simplicity. It's  And although it is performing well on the training and the test set, it is the least fitting one. Luckily, the second fastest model (GBR) is also the best model in terms of test accuracy, which is why, we will go with that model for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning of the best model: Define hyperparameters to be tuned\n",
    "param_grid = {'GradientBoosting__learning_rate':[0.001,0.005,0.01,0.1],\n",
    "              'GradientBoosting__n_estimators':[50,100,200],\n",
    "              'GradientBoosting__max_depth':[1,3,5]}\n",
    "\n",
    "grad_boost_pipe_cv = GridSearchCV(grad_boost_pipe,param_grid=param_grid,cv=3,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning of the best model: Fit final model\n",
    "grad_boost_pipe_cv.fit(X_train,y_train)\n",
    "\n",
    "final_results = test_model(grad_boost_pipe_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning of the best model: Display final results\n",
    "print('Total Runtime: {}'.format(final_results[0]))\n",
    "print('Training Score (RMSLE): {}'.format(final_results[1]))\n",
    "print('Test Score (RMSLE) {}'.format(final_results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model versus Deep Learning Network: Special pipeline\n",
    "dln_pipe = Pipeline([('OneHotEncoder',encoder),\n",
    "                     ('Normalizer',norm),\n",
    "                     ('StandardScaler',scaler),\n",
    "                     ('TruncatedSVD',TruncatedSVD(n_components=7))])\n",
    "\n",
    "input_train = dln_pipe.fit_transform(X_train)\n",
    "labels_train = y_train.reshape(-1,1)\n",
    "input_test = dln_pipe.fit_transform(X_test)\n",
    "labels_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128,kernel_initializer='normal',input_dim=input_train.shape[1],activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "model.compile(loss = 'mean_absolute_error',optimizer='adam',metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_train,labels_train,epochs=500,batch_size=32,validation_split=0.2,verbose=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(input_test)\n",
    "dln_score = rmsle(labels_test,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare results of best untuned ML model, best tuned ML model and deep learning network.\n",
    "print('Best untuned ML model: {}'.format(results[-1:-1]))\n",
    "print('Best tuned ML model: {}'.format(final_results[-1]))\n",
    "print('Best Deep Learning model: {}'.format(dln_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use test_processed.csv to make submission-ready predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
